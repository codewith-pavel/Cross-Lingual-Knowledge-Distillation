{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10538946,"sourceType":"datasetVersion","datasetId":6520921},{"sourceId":255507,"sourceType":"modelInstanceVersion","modelInstanceId":218440,"modelId":240168},{"sourceId":255801,"sourceType":"modelInstanceVersion","modelInstanceId":218686,"modelId":240420},{"sourceId":256148,"sourceType":"modelInstanceVersion","modelInstanceId":218988,"modelId":240742},{"sourceId":256727,"sourceType":"modelInstanceVersion","modelInstanceId":219449,"modelId":241202}],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"papermill":{"default_parameters":{},"duration":18934.447804,"end_time":"2024-09-30T10:31:01.641427","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-09-30T05:15:27.193623","version":"2.6.0"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"00f5e5c945994ab2a25802e05d2b74c1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2958a273a5624c74aeef53b52b8ecbe5","placeholder":"​","style":"IPY_MODEL_33d9e38d438b481a8fb3359298f96904","value":"pytorch_model.bin: 100%"}},"106ff11ec3a440c29bddca56b1c6ef80":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_a52e37cf59af41e682e9d9b294365b70","IPY_MODEL_b9f675dafc324fa985f9abf5af23736e","IPY_MODEL_5c5022b65cd04324b895be5d08f2b8fe"],"layout":"IPY_MODEL_3eccdf38dc364d3d8e9db7f2b4314f99"}},"1bc4f342965140218c5c2b4eefa29286":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"262338affce944bba865e2946afb9d9a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2958a273a5624c74aeef53b52b8ecbe5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"33d9e38d438b481a8fb3359298f96904":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"35999fb0e0794d4b806d17309f01342c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3eccdf38dc364d3d8e9db7f2b4314f99":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"48663661d7c44190b875ad18ca065ff2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"49cb0174ee734deabbd46d3a8f724848":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5359444cbeac42f29b49966d3217ea8e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5c5022b65cd04324b895be5d08f2b8fe":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce232a6066274406a0d111ecddf2ef0e","placeholder":"​","style":"IPY_MODEL_f1a528bee1414ef1bdd476cf630f7a28","value":" 428/428 [00:00&lt;00:00, 32.5kB/s]"}},"5cb08ec53da04871bc97853a1dbd4dde":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_262338affce944bba865e2946afb9d9a","placeholder":"​","style":"IPY_MODEL_35999fb0e0794d4b806d17309f01342c","value":"vocab.txt: 100%"}},"60020e6f1fdf42a3a56da4c9addce6ab":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"669c89ef40bd483c82699e0bf4fdd1fb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"677c278f393f4103bb4b0292f09ba27b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_00f5e5c945994ab2a25802e05d2b74c1","IPY_MODEL_ef563e01f3b748da84a4751f21e2ca92","IPY_MODEL_ca563ba85cf54b5a8eae8c3795ccd67a"],"layout":"IPY_MODEL_48663661d7c44190b875ad18ca065ff2"}},"6ac71724b98a4f448bf50838b53fdfff":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"73e0743b473144e09c9cf04d3867ed52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_5cb08ec53da04871bc97853a1dbd4dde","IPY_MODEL_fd894b54afb245cba0873619035433f4","IPY_MODEL_9a61ec5bd2c34cef9bf464cf6e37ac96"],"layout":"IPY_MODEL_e645d0a464e74a699265668fb48d3e0f"}},"9a61ec5bd2c34cef9bf464cf6e37ac96":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_fab1aebcb18c427e9de4a714019bdfc2","placeholder":"​","style":"IPY_MODEL_1bc4f342965140218c5c2b4eefa29286","value":" 213k/213k [00:00&lt;00:00, 2.98MB/s]"}},"a52e37cf59af41e682e9d9b294365b70":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_49cb0174ee734deabbd46d3a8f724848","placeholder":"​","style":"IPY_MODEL_f72b853b1be64e7792cdd682ec483c2b","value":"config.json: 100%"}},"b4a11ef429b54fd4a478e3a952e5d446":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b9f675dafc324fa985f9abf5af23736e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f0b995ab4576453986d0904e4dae3bf7","max":428,"min":0,"orientation":"horizontal","style":"IPY_MODEL_5359444cbeac42f29b49966d3217ea8e","value":428}},"c92c75fb4f9146d9ac5a0feb9cf104b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca563ba85cf54b5a8eae8c3795ccd67a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_669c89ef40bd483c82699e0bf4fdd1fb","placeholder":"​","style":"IPY_MODEL_db82597a952e40ddae0f7507ae308a6f","value":" 433M/433M [00:01&lt;00:00, 296MB/s]"}},"ce232a6066274406a0d111ecddf2ef0e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"db82597a952e40ddae0f7507ae308a6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e645d0a464e74a699265668fb48d3e0f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ef563e01f3b748da84a4751f21e2ca92":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c92c75fb4f9146d9ac5a0feb9cf104b5","max":433289285,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ac71724b98a4f448bf50838b53fdfff","value":433289285}},"f0b995ab4576453986d0904e4dae3bf7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1a528bee1414ef1bdd476cf630f7a28":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f72b853b1be64e7792cdd682ec483c2b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fab1aebcb18c427e9de4a714019bdfc2":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd894b54afb245cba0873619035433f4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4a11ef429b54fd4a478e3a952e5d446","max":213330,"min":0,"orientation":"horizontal","style":"IPY_MODEL_60020e6f1fdf42a3a56da4c9addce6ab","value":213330}}},"version_major":2,"version_minor":0}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader\nfrom transformers import BertForSequenceClassification, BertTokenizer, AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nimport pandas as pd\nimport numpy as np\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments","metadata":{"papermill":{"duration":19.564397,"end_time":"2024-09-30T05:15:49.447237","exception":false,"start_time":"2024-09-30T05:15:29.88284","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load Train, Validation, Test Dataset\ntrain_dataset = pd.read_csv('/kaggle/input/clinical-datasets/Google_Translated/Google_Translated/MIMIC_III/MP_IN_adm_train.csv')\nval_dataset = pd.read_csv('/kaggle/input/clinical-datasets/Google_Translated/Google_Translated/MIMIC_III/MP_IN_adm_val.csv')\ntest_dataset = pd.read_csv('/kaggle/input/clinical-datasets/Google_Translated/Google_Translated/MIMIC_III/MP_IN_adm_test.csv')","metadata":{"papermill":{"duration":1.893175,"end_time":"2024-09-30T05:15:51.346997","exception":false,"start_time":"2024-09-30T05:15:49.453822","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Create subplots\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\n# Function to add count labels above bars\ndef add_count_labels(ax):\n    for p in ax.patches:\n        height = p.get_height()\n        ax.text(p.get_x() + p.get_width() / 2., height + 0.5,  # Positioning the text\n                f'{int(height)}', ha='center', va='bottom', fontsize=10)\n\n# Train Set\nsns.countplot(x='hospital_expire_flag', data=train_dataset, ax=axes[0])\naxes[0].set_title(\"Train Dataset Class Distribution\")\nadd_count_labels(axes[0])\n\n# Validation Set\nsns.countplot(x='hospital_expire_flag', data=val_dataset, ax=axes[1])\naxes[1].set_title(\"Validation Dataset Class Distribution\")\nadd_count_labels(axes[1])\n\n# Test Set\nsns.countplot(x='hospital_expire_flag', data=test_dataset, ax=axes[2])\naxes[2].set_title(\"Test Dataset Class Distribution\")\nadd_count_labels(axes[2])\n\n# Display the plots\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\nclass EnsembleModel(nn.Module):\n    def __init__(self, model1, model2):\n        super(EnsembleModel, self).__init__()\n        self.model1 = model1\n        self.model2 = model2\n        \n        # Initial attention scores (for learning importance of each model)\n        self.attention1 = nn.Parameter(torch.tensor(1.0))  # Model 1 attention\n        self.attention2 = nn.Parameter(torch.tensor(1.0))  # Model 2 attention\n        \n        # Model weight parameters (initialized to 1 for simplicity)\n        self.weight1 = nn.Parameter(torch.tensor(1.0))\n        self.weight2 = nn.Parameter(torch.tensor(1.0))\n        \n        # Parameters to keep track of performance (boosting mechanism)\n        self.alpha1 = nn.Parameter(torch.tensor(0.5))\n        self.alpha2 = nn.Parameter(torch.tensor(0.5))\n    \n    def forward(self, input_ids, attention_mask, loss1=None, loss2=None):\n        # Get the model outputs\n        outputs1 = self.model1(input_ids, attention_mask=attention_mask)[0]\n        outputs2 = self.model2(input_ids, attention_mask=attention_mask)[0]\n        \n        # Apply softmax to normalize the model weights\n        weights = torch.softmax(torch.stack([self.weight1, self.weight2]), dim=0)\n        \n        # Apply softmax to normalize the attention weights (to focus on the important model outputs)\n        attention_weights = torch.softmax(torch.stack([self.attention1, self.attention2]), dim=0)\n        \n        # Weighted sum of the model outputs (with attention)\n        weighted_output = attention_weights[0] * (weights[0] * outputs1) + attention_weights[1] * (weights[1] * outputs2)\n        \n        # Boosting: Update model weights based on model performance\n        if loss1 is not None and loss2 is not None:\n            # For simplicity, assume boosting updates are based on model performance\n            # (i.e., inverse of loss) to increase focus on harder examples\n            self.alpha1.data = self.alpha1.data * torch.exp(-loss1)\n            self.alpha2.data = self.alpha2.data * torch.exp(-loss2)\n\n            # Adjust model weights based on boosting factors\n            self.weight1.data = self.weight1.data + self.alpha1\n            self.weight2.data = self.weight2.data + self.alpha2\n\n        return weighted_output","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoConfig\n\n# Specify the dropout rate in the configuration\nconfig = AutoConfig.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', \n                                    num_labels=2, \n                                    hidden_dropout_prob=0.2, \n                                    attention_probs_dropout_prob=0.2)\n\n# Load the pre-trained model with the specified configuration\nmodel1 = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_ClinicalBERT', config=config)","metadata":{"papermill":{"duration":2.955371,"end_time":"2024-09-30T05:15:54.327588","exception":false,"start_time":"2024-09-30T05:15:51.372217","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoConfig\n\n# Specify the dropout rate in the configuration\nconfig = AutoConfig.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT', \n                                    num_labels=2, \n                                    hidden_dropout_prob=0.2, \n                                    attention_probs_dropout_prob=0.2)\n\n# Load the pre-trained model with the specified configuration\nmodel2 = AutoModelForSequenceClassification.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT', config=config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Choose a tokenizer\ntokenizer = AutoTokenizer.from_pretrained('emilyalsentzer/Bio_Discharge_Summary_BERT')","metadata":{"papermill":{"duration":0.854094,"end_time":"2024-09-30T05:15:55.245653","exception":false,"start_time":"2024-09-30T05:15:54.391559","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = train_dataset.dropna(subset=['text']).reset_index(drop=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Apply the tokenizer to the training, validation, and test datasets\ntrain_encodings = tokenizer(train_dataset['text'].tolist(), truncation=True, padding=True, max_length = 512)\nval_encodings = tokenizer(val_dataset['text'].tolist(), truncation=True, padding=True,  max_length = 512)\ntest_encodings = tokenizer(test_dataset['text'].tolist(), truncation=True, padding=True , max_length = 512)","metadata":{"papermill":{"duration":36.095414,"end_time":"2024-09-30T05:16:31.347676","exception":false,"start_time":"2024-09-30T05:15:55.252262","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Create a Dataset for PyTorch\nclass LosDataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"papermill":{"duration":0.01585,"end_time":"2024-09-30T05:16:31.370739","exception":false,"start_time":"2024-09-30T05:16:31.354889","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_dataset = LosDataset(train_encodings, train_dataset['hospital_expire_flag'].tolist())\nval_dataset = LosDataset(val_encodings, val_dataset['hospital_expire_flag'].tolist())\ntest_dataset = LosDataset(test_encodings, test_dataset['hospital_expire_flag'].tolist())","metadata":{"papermill":{"duration":0.02071,"end_time":"2024-09-30T05:16:31.397754","exception":false,"start_time":"2024-09-30T05:16:31.377044","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\nfrom tqdm import tqdm\nfrom torch import nn\nimport numpy as np\n\n# Initialize the Ensemble Model\nensemble_model = EnsembleModel(model1, model2)","metadata":{"papermill":{"duration":0.013838,"end_time":"2024-09-30T05:16:31.417976","exception":false,"start_time":"2024-09-30T05:16:31.404138","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n        \n# get the first (and supposedly only) model\nmodel_path = \"/kaggle/input/mp_teacher/pytorch/default/1/MP_ENEMBLE_TEACHER_epoch_3_roc_0.8658.pth\"\n# load the model state\nensemble_model.load_state_dict(torch.load(model_path))\nprint(f\"Loaded Model: {model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"ensemble_model","metadata":{"papermill":{"duration":0.012806,"end_time":"2024-09-30T05:16:31.459185","exception":false,"start_time":"2024-09-30T05:16:31.446379","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'MP_ENEMBLE_TEACHER'\ncore_models = [f for f in files if f.startswith('MP_ENEMBLE_TEACHER')]\n\nif core_models:\n    print(\"Found models starting with 'MP_ENEMBLE_TEACHER':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[1]\n\n    # load the model state\n    ensemble_model.load_state_dict(torch.load(model_path))\n    print(\"Loaded Model\")\nelse:\n    print(\"No models found starting with 'MP_ENEMBLE_TEACHER'.\")","metadata":{"papermill":{"duration":0.015529,"end_time":"2024-09-30T05:16:31.439862","exception":false,"start_time":"2024-09-30T05:16:31.424333","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Push the model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nensemble_model = ensemble_model.to(device)","metadata":{"papermill":{"duration":0.348279,"end_time":"2024-09-30T05:16:31.814009","exception":false,"start_time":"2024-09-30T05:16:31.46573","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\ntest_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)","metadata":{"papermill":{"duration":0.014851,"end_time":"2024-09-30T05:16:31.835874","exception":false,"start_time":"2024-09-30T05:16:31.821023","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 100\nbest_roc_auc = 0\nmin_delta = 0.0001\nearly_stopping_count = 0\nearly_stopping_patience = 5\ngradient_accumulation_steps = 10\nbest_model_path = \"best_model.pth\"\n\n# Set the optimizer\noptimizer = AdamW(ensemble_model.parameters(), lr=1e-5, weight_decay=0.01)\n\n# Set the scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=50, \n    num_training_steps=len(train_loader) * epochs // gradient_accumulation_steps\n)\n","metadata":{"papermill":{"duration":0.015027,"end_time":"2024-09-30T05:16:32.294291","exception":false,"start_time":"2024-09-30T05:16:32.279264","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\nimport torch.nn as nn\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\n\n# Compute class weights\nclasses = np.unique(train_dataset.labels)  # Get unique class labels\nclass_weights = compute_class_weight('balanced', classes=classes, y=train_dataset.labels)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n\n# Print class weights with corresponding class names\nprint(\"Class Weights:\")\nfor cls, weight in zip(classes, class_weights):\n    print(f\"Class {cls}: Weight {weight}\")\n\nroc_auc_values = []\n# Training loop\nfor epoch in range(0, epochs):\n    ensemble_model.train()\n    train_loss = 0\n    for step, batch in enumerate(tqdm(train_loader)):\n        optimizer.zero_grad() if step % gradient_accumulation_steps == 0 else None\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs1 = ensemble_model.model1(input_ids, attention_mask=attention_mask)[0]\n        outputs2 = ensemble_model.model2(input_ids, attention_mask=attention_mask)[0]\n        \n        # Compute individual losses\n        loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n        loss1 = loss_fn(outputs1, labels)\n        loss2 = loss_fn(outputs2, labels)\n        \n        ensemble_output = ensemble_model(input_ids, attention_mask, loss1=loss1, loss2=loss2)\n        \n        loss = loss_fn(ensemble_output, labels)\n\n        (loss / gradient_accumulation_steps).backward()\n        train_loss += loss.item()\n        \n        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n            optimizer.step()\n            scheduler.step()\n\n    # Validation\n    ensemble_model.eval()\n    val_loss = 0\n    val_preds = []\n    val_labels = []\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n\n            outputs1 = ensemble_model.model1(input_ids, attention_mask=attention_mask)[0]\n            outputs2 = ensemble_model.model2(input_ids, attention_mask=attention_mask)[0]\n            # Compute individual losses\n            loss_fn = nn.CrossEntropyLoss(weight=class_weights_tensor)\n            loss1 = loss_fn(outputs1, labels)\n            loss2 = loss_fn(outputs2, labels)\n        \n            ensemble_output = ensemble_model(input_ids, attention_mask, loss1=loss1, loss2=loss2)\n\n            loss = loss_fn(ensemble_output, labels)\n            val_loss += loss.item()\n\n            val_preds.append(F.softmax(ensemble_output, dim=1).cpu().numpy())\n            val_labels.append(labels.cpu().numpy())\n\n    val_preds = np.concatenate(val_preds)\n    val_labels = np.concatenate(val_labels)\n    val_loss /= len(val_loader)\n    train_loss /= len(train_loader)\n\n    print(f'Epoch: {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n\n    # Calculate evaluation metrics\n    val_preds_class = np.argmax(val_preds, axis=1)\n    accuracy = accuracy_score(val_labels, val_preds_class) \n    recall = recall_score(val_labels, val_preds_class) \n    precision = precision_score(val_labels, val_preds_class) \n    f1 = f1_score(val_labels, val_preds_class) \n    micro_f1 = f1_score(val_labels, val_preds_class) \n    macro_roc_auc = roc_auc_score(val_labels, val_preds[:, 1]) \n\n    print(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')\n    # Store the ROC-AUC value for this epoch\n    roc_auc_values.append(macro_roc_auc)\n\n    # Implement early stopping\n    if epoch > 0 and macro_roc_auc - best_roc_auc < min_delta:\n        early_stopping_count += 1\n        print(f'EarlyStopping counter: {early_stopping_count} out of {early_stopping_patience}')\n        if early_stopping_count >= early_stopping_patience:\n            print('Early stopping')\n            break\n    else:\n        best_roc_auc = macro_roc_auc\n        early_stopping_count = 0\n        torch.save(ensemble_model.state_dict(), f\"MP_ENEMBLE_TEACHER_epoch_{epoch}_roc_{best_roc_auc:.4f}.pth\")\n\n# After training, plot the ROC-AUC curve\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(roc_auc_values) + 1), roc_auc_values, marker='o', linestyle='-', color='b', label='ROC-AUC Score')\n\n# Enhancing readability\nplt.title('ROC-AUC Score Progression', fontsize=14, fontweight='bold')\nplt.xlabel('Epoch', fontsize=12)\nplt.ylabel('ROC-AUC Score', fontsize=12)\nplt.xticks(fontsize=10)\nplt.yticks(fontsize=10)\nplt.grid(True, linestyle='--', alpha=0.6)\nplt.legend(fontsize=10, loc='lower right')\nplt.ylim(0.7, 1.0)  # Adjust y-axis range if necessary\n\nplt.show()\n\n# Convert the list of ROC-AUC values and epochs to a DataFrame\nroc_auc_df = pd.DataFrame(roc_auc_values)\n\n# Save the DataFrame to a CSV file\nroc_auc_df.to_csv('roc_auc_per_epoch.csv', index=False)\n\nprint(\"ROC-AUC values per epoch saved to 'roc_auc_per_epoch.csv'\")","metadata":{"papermill":{"duration":18693.303378,"end_time":"2024-09-30T10:28:05.604454","exception":false,"start_time":"2024-09-30T05:16:32.301076","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'MP_ENEMBLE_TEACHER'\ncore_models = sorted([f for f in files if f.startswith('MP_ENEMBLE_TEACHER')])\n\nif core_models:\n    print(\"Found models starting with 'MP_ENEMBLE_TEACHER':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[-1]\n\n    # load the model state\n    ensemble_model.load_state_dict(torch.load(model_path))\n    print(\"Loaded Model\")\nelse:\n    print(\"No models found starting with 'MP_ENEMBLE_TEACHER'.\")","metadata":{"papermill":{"duration":4.324644,"end_time":"2024-09-30T10:28:13.933254","exception":false,"start_time":"2024-09-30T10:28:09.60861","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Put the model in evaluation mode\nensemble_model.eval()\n\n# Initialize lists to store predictions and true labels\ntest_preds = []\ntest_labels = []\n\n# Iterate over test data\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = ensemble_model(input_ids, attention_mask)\n        test_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n        test_labels.append(labels.cpu().numpy())\n\n","metadata":{"papermill":{"duration":140.574652,"end_time":"2024-09-30T10:30:38.384025","exception":false,"start_time":"2024-09-30T10:28:17.809373","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds = np.concatenate(test_preds)\ntest_labels = np.concatenate(test_labels)\n\n# Calculate metrics\ntest_preds_class = np.argmax(test_preds, axis=1)\naccuracy = accuracy_score(test_labels, test_preds_class) \nrecall = recall_score(test_labels, test_preds_class) \nprecision = precision_score(test_labels, test_preds_class) \nf1 = f1_score(test_labels, test_preds_class) \nmicro_f1 = f1_score(test_labels, test_preds_class) \nmacro_roc_auc = roc_auc_score(test_labels, test_preds[:, 1]) \n\nprint(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')","metadata":{"papermill":{"duration":4.029713,"end_time":"2024-09-30T10:30:46.458264","exception":false,"start_time":"2024-09-30T10:30:42.428551","status":"completed"},"tags":[],"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification, AutoConfig\n\n# Specify the dropout rate in the configuration\nconfig = AutoConfig.from_pretrained('FacebookAI/xlm-roberta-base', \n                                    num_labels=2, \n                                    hidden_dropout_prob=0.2, \n                                    attention_probs_dropout_prob=0.2)\n\n# Load the pre-trained model with the specified configuration\nintermediate_model = AutoModelForSequenceClassification.from_pretrained('FacebookAI/xlm-roberta-base', config=config)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\n# Choose a tokenizer\ntokenizer = AutoTokenizer.from_pretrained('FacebookAI/xlm-roberta-base')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'MP_INTEMEDIATE'\ncore_models = sorted([f for f in files if f.startswith('MP_INTEMEDIATE')])\n\nif core_models:\n    print(\"Found models starting with 'MP_INTEMEDIATE':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[-1]\n\n    # load the model state\n    ensemble_model.load_state_dict(torch.load(model_path))\n    print(\"Loaded Model\")\nelse:\n    print(\"No models found starting with 'MP_INTEMEDIATE'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n        \n# get the first (and supposedly only) model\nmodel_path = \"/kaggle/input/mp-train-3/pytorch/default/1/MP_INTEMEDIATE_epoch_17_roc_0.8588602884908751_alpha_1.0_temperature_2.0906016760958854.pth\"\n# load the model state\nintermediate_model.load_state_dict(torch.load(model_path))\nprint(f\"Loaded Model: {model_path}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Push the model to device\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nintermediate_model = intermediate_model.to(device)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"epochs = 30\nbest_roc_auc = 0.8588602884908751\nmin_delta = 0.0001\nearly_stopping_count = 0\nearly_stopping_patience = 5\ngradient_accumulation_steps = 10\nbest_model_path = \"best_model.pth\"\n\n# Set the optimizer\noptimizer = AdamW(intermediate_model.parameters(), lr=1e-5, weight_decay=0.01)\n\n# Set the scheduler\nscheduler = get_linear_schedule_with_warmup(\n    optimizer, \n    num_warmup_steps=50, \n    num_training_steps=len(train_loader) * epochs // gradient_accumulation_steps\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nfrom torch.nn import functional as F\nimport torch.nn as nn\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score, roc_auc_score\n\n# Set initial values\ncurrent_alpha = 1 # Start with a low alpha\ncurrent_temperature = 2.0 # Start with a high temperature\n\nroc_auc_values = []\nalpha_values = []\ntemperature_values = []\n\nensemble_model.eval()\n\n# Compute class weights\nclasses = np.unique(train_dataset.labels)\nclass_weights = compute_class_weight('balanced', classes=classes, y=train_dataset.labels)\nclass_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n\nprint(\"Class Weights:\")\nfor cls, weight in zip(classes, class_weights):\n    print(f\"Class {cls}: Weight {weight}\")\n\n# Training Loop\nfor epoch in range(20, epochs):\n    intermediate_model.train()\n    train_loss = 0\n\n    for step, batch in enumerate(tqdm(train_loader)):\n        optimizer.zero_grad() if step % gradient_accumulation_steps == 0 else None\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        # Get intermediate model's logits\n        intermediate_logits = intermediate_model(input_ids, attention_mask)[0]\n\n        # Get teacher model's logits\n        with torch.no_grad():\n            teacher_logits = ensemble_model(input_ids, attention_mask)\n\n        # Compute KL loss\n        kl_loss = nn.KLDivLoss(reduction=\"batchmean\")(\n            F.log_softmax(intermediate_logits / current_temperature, dim=1),\n            F.softmax(teacher_logits / current_temperature, dim=1)\n        ) * (current_temperature ** 2)\n\n        # Compute Cross-Entropy loss\n        ce_loss = nn.CrossEntropyLoss(weight=class_weights_tensor)(intermediate_logits, labels)\n\n        # Combine losses with current alpha\n        loss = current_alpha * kl_loss + (1 - current_alpha) * ce_loss\n\n        (loss / gradient_accumulation_steps).backward()\n\n        train_loss += loss.item()\n\n        if (step + 1) % gradient_accumulation_steps == 0 or (step + 1) == len(train_loader):\n            optimizer.step()\n            scheduler.step()\n\n    # Evaluation\n    intermediate_model.eval()\n    val_loss = 0\n    val_preds = []\n    val_labels = []\n\n    with torch.no_grad():\n        for batch in tqdm(val_loader):\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['labels'].to(device)\n            outputs = intermediate_model(input_ids, attention_mask)[0]\n            loss = nn.CrossEntropyLoss(weight=class_weights_tensor)(outputs, labels)\n            val_loss += loss.item()\n            val_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n            val_labels.append(labels.cpu().numpy())\n\n    val_preds = np.concatenate(val_preds)\n    val_labels = np.concatenate(val_labels)\n    val_loss /= len(val_loader)\n    train_loss /= len(train_loader)\n\n    # Calculate Metrics\n    val_preds_class = np.argmax(val_preds, axis=1)\n    accuracy = accuracy_score(val_labels, val_preds_class) \n    recall = recall_score(val_labels, val_preds_class) \n    precision = precision_score(val_labels, val_preds_class) \n    f1 = f1_score(val_labels, val_preds_class) \n    micro_f1 = f1_score(val_labels, val_preds_class) \n    macro_roc_auc = roc_auc_score(val_labels, val_preds[:, 1]) \n\n    print(f'Epoch: {epoch+1}/{epochs}, Training Loss: {train_loss}, Validation Loss: {val_loss}')\n    print(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc AUC: {macro_roc_auc}')\n    print(f'With Alpha: {current_alpha}, Temperature: {current_temperature}')\n\n    # Store values for plotting\n    roc_auc_values.append(macro_roc_auc)\n    alpha_values.append(current_alpha)\n    temperature_values.append(current_temperature)\n\n    # Implement early stopping\n    if macro_roc_auc - best_roc_auc < min_delta:\n        early_stopping_count += 1\n        print(f'EarlyStopping counter: {early_stopping_count} out of {early_stopping_patience}')\n        if early_stopping_count >= early_stopping_patience:\n            print('Early stopping')\n            break\n    else:\n        best_roc_auc = macro_roc_auc\n        early_stopping_count = 0\n        torch.save(intermediate_model.state_dict(), f\"MP_INTEMEDIATE_epoch_{epoch}_roc_{best_roc_auc}_alpha_{current_alpha}_temperature_{current_temperature}.pth\")\n    \n    # Update alpha and temperature\n    current_temperature = max(2, current_temperature * 0.95)  # Decrease temp gradually (min 2)\n    current_alpha = min(1.0, current_alpha + 0.05)  # Increase alpha gradually (max 1.0)\n    \n# Plot ROC-AUC over epochs\nplt.figure(figsize=(8, 5))\nplt.plot(range(1, len(roc_auc_values) + 1), roc_auc_values, marker='o', linestyle='-', color='b', label='ROC-AUC Score')\nplt.xlabel('Epoch')\nplt.ylabel('ROC-AUC Score')\nplt.title('ROC-AUC Score Progression')\nplt.grid(True)\nplt.legend()\nplt.savefig('roc_auc_vs_epochs.png')\nplt.show()\n\n# Plot ROC-AUC vs Alpha\nplt.figure(figsize=(8, 5))\nplt.plot(alpha_values, roc_auc_values, marker='o', linestyle='-', color='r', label='ROC-AUC vs Alpha')\nplt.xlabel('Alpha')\nplt.ylabel('ROC-AUC Score')\nplt.title('ROC-AUC Score vs Alpha')\nplt.grid(True)\nplt.legend()\nplt.savefig('roc_auc_vs_alpha.png')\nplt.show()\n\n# Plot ROC-AUC vs Temperature\nplt.figure(figsize=(8, 5))\nplt.plot(temperature_values, roc_auc_values, marker='o', linestyle='-', color='g', label='ROC-AUC vs Temperature')\nplt.xlabel('Temperature')\nplt.ylabel('ROC-AUC Score')\nplt.title('ROC-AUC Score vs Temperature')\nplt.grid(True)\nplt.legend()\nplt.savefig('roc_auc_vs_temperature.png')\nplt.show()\n\n# Save values to CSV\ndf = pd.DataFrame({\n    'Epoch': list(range(1, len(roc_auc_values) + 1)),\n    'ROC_AUC': roc_auc_values,\n    'Alpha': alpha_values,\n    'Temperature': temperature_values\n})\ndf.to_csv('roc_auc_alpha_temperature.csv', index=False)\nprint(\"All values saved successfully!\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\n# list all files in the current directory\nfiles = os.listdir('.')\n\n# filter the ones that start with 'MP_INTEMEDIATE'\ncore_models = sorted([f for f in files if f.startswith('MP_INTEMEDIATE')])\n\nif core_models:\n    print(\"Found models starting with 'MP_INTEMEDIATE':\")\n    for model in core_models:\n        print(model)\n        \n    # get the first (and supposedly only) model\n    model_path = core_models[-1]\n\n    # load the model state\n    intermediate_model.load_state_dict(torch.load(model_path))\n    print(\"Loaded Model\")\nelse:\n    print(\"No models found starting with 'MP_INTEMEDIATE'.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Put the model in evaluation mode\nintermediate_model.eval()\n\n# Initialize lists to store predictions and true labels\ntest_preds = []\ntest_labels = []\n\n# Iterate over test data\nwith torch.no_grad():\n    for batch in tqdm(test_loader):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = intermediate_model(input_ids, attention_mask)[0]\n        test_preds.append(F.softmax(outputs, dim=1).cpu().numpy())\n        test_labels.append(labels.cpu().numpy())","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_preds = np.concatenate(test_preds)\ntest_labels = np.concatenate(test_labels)\n\n# Calculate metrics\ntest_preds_class = np.argmax(test_preds, axis=1)\naccuracy = accuracy_score(test_labels, test_preds_class) \nrecall = recall_score(test_labels, test_preds_class) \nprecision = precision_score(test_labels, test_preds_class) \nf1 = f1_score(test_labels, test_preds_class) \nmicro_f1 = f1_score(test_labels, test_preds_class) \nmacro_roc_auc = roc_auc_score(test_labels, test_preds[:, 1]) \n\nprint(f'Accuracy: {accuracy}, Recall: {recall}, Precision: {precision}, F1: {f1}, Micro F1: {micro_f1}, Macro Roc Auc: {macro_roc_auc}')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}